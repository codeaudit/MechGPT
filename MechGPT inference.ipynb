{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d1c3fa-b23f-4c32-b385-4e42805dd57c",
   "metadata": {},
   "source": [
    "### MechGPT inference "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ea02028-2714-4b12-b764-2b5d35aa7c72",
   "metadata": {},
   "source": [
    "M.J. Buehler, 2023\n",
    "mbuehler@MIT.EDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab0e9f-56ca-4e49-a9f1-47accc76ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    " \n",
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "from transformers import  TextIteratorStreamer\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "import gradio as gr\n",
    " \n",
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "model_name='Open-Orca/OpenOrca-Platypus2-13B'\n",
    "FT_model_name = 'MechGPT-13b_v106C'\n",
    "\n",
    "peft_model_id = f'{FT_model_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ed6a5-9901-4af0-a2ba-bb3edb327cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config= bnb_config4bit,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6792d-3985-4369-85e3-ebc957cc42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base.config.use_cache = False\n",
    "model = PeftModel.from_pretrained(model_base, peft_model_id, \n",
    "                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf2c90-5528-4606-830e-b405eb3f8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "def generate_response (text_input=\"Mechanics is a powerful discipline with many applications, such as \",\n",
    "                      num_return_sequences=1,\n",
    "                      temperature=0.4, #the higher the temperature, the more creative the model becomes\n",
    "                      max_new_tokens=128,\n",
    "                      num_beams=1,\n",
    "                      top_k = 50,\n",
    "                      top_p = 0.9,\n",
    "                      repetition_penalty=1.,eos_token_id=2,verbatim=False,\n",
    "                     ):\n",
    "\n",
    "    inputs = tokenizer.encode(text_input,  add_special_tokens  =False,  return_tensors ='pt')\n",
    "    if verbatim:\n",
    "        print (\"Length of input, tokenized: \", inputs.shape)\n",
    "    with torch.no_grad():\n",
    "          outputs = model.generate(input_ids=inputs.to(device), \n",
    "                                   max_new_tokens=max_new_tokens,\n",
    "                                   temperature=temperature, \n",
    "                                   num_beams=num_beams,\n",
    "                                   top_k = top_k,\n",
    "                                   top_p =top_p,\n",
    "                                   num_return_sequences = num_return_sequences, eos_token_id=eos_token_id,\n",
    "                                   do_sample =True,\n",
    "                                   repetition_penalty=repetition_penalty,\n",
    "                                  )\n",
    "    return tokenizer.batch_decode(outputs[:,inputs.shape[1]:].detach().cpu().numpy(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e67b25-f5ce-45a8-899f-59cc5f854a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer ('<|end_of_turn|></s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fb401-bc8d-4a7f-ba4f-9fb5c61aa341",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response (    text_input=\"How does hyperelastic softening affect crack speed in brittle materials?\",\n",
    "                       max_new_tokens=128,\n",
    "                       temperature=0.3, #value used to modulate the next token probabilities.\n",
    "                       num_beams=1,\n",
    "                       top_k = 50,\n",
    "                       top_p = 0.9,\n",
    "                       num_return_sequences = 1, eos_token_id=[2, 32000],\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c073889-dd07-4e91-b606-054011028509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
